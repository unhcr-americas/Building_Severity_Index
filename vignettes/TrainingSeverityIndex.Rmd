---
title: "Building Humanitarian Severity Index with R & COINr"
subtitle: "Esnure Statistical Rigor & Robustness"
author: " "
institute: ""
date: "Last update: `r Sys.Date()`"
output:
  xaringan::moon_reader:
    lib_dir: libs
    nature:
      highlightStyle: github
      highlightLines: true
      countIncrementalSlides: false
      ratio: 16:9
    css: ["unhcr-slides.css", "unhcr-slides-fonts.css"]
    includes:
      in_header: header.html    
---
layout: true

<div class="my-footer"><span></span></div> 



```{r setup, include=FALSE}
options(htmltools.dir.version = FALSE)
library(knitr)
knitr::opts_chunk$set(cache = TRUE, 
                      warning = FALSE, 
                      message = FALSE, 
                      dpi = 180,
                      fig.retina = 3,
                      fig.width = 6,
                      fig.asp = 0.618
                      )
#'UNHCR / Americas Bureau /DIMA  <br> <a href="http://github.com/unhcr-americas"><i class="fa fa-github fa-fw"></i>&nbsp; unhcr-americas</a><br> <br><br> See previous Slides available at <https://unhcr-americas.github.io/reproducibility> <br>'

```

---

# Humanitarian Severity Index



.pull-left[

Humanitarian response actors evaluate crisis severity through one consolidated index or multiple sectoral indices.

Each index acts as a metric to quantify and to compare the situation of different admin units within a country. 

Crisis severity is indeed a complex, multi-factorial construct 


CHALLENGE: Summarising and __condensing__ the information of a plurality of underlying indicators into a single measure, in a way that __accurately__ reflects the underlying concept

 
]

 
.pull-right[
 
Example from JRC Global [INFORM]((http://www.inform-index.org)), an index designed to assess humanitarian risks for 191 countries at the national level. 


![Risk](images/inform.png)  


]

???
Depending on the pace of the group, if we do not finish today we will org anise a second session 

---


## The risk of poorly statistically sound index 

.pull-left[

Conceptual and statistical flaws in the calculation of Humanitarian Severity can greatly limit its usefulness
 
 * Are indicators correctly depicting the severity?
 
 * Are all indicators "really useful" to the final measurement?
  
 * Does the way the indicators are 'assembled" together correctly reflect their true respective importance?

 * Can the methodology be fully __positively audited__?
 
]

 
.pull-right[
 


![Risk](images/risk.png)  


]


 



???
Composite indicators are developed to address the challenge that comes with the aggregation of heterogeneous information. They are developed to convey consistent policy messages
Because of their very nature, composite indicators cannot be validated versus a ground truth and are always a compromise and as a result tends to create polarized audiences.

Some cons:
Composite indicators may send misleading, non-robust policy messages if they are poorly constructed or misinterpreted. Sensitivity analysis can be used to test composite indicators for robustness.
The simple “big picture” results which composite indicators show may invite politicians to draw simplistic policy conclusions. Composite indicators should be used in combination with the sub-indicators to draw sophisticated policy conclusions.
The construction of composite indicators involves stages where judgement has to be made: the selection of sub-indicators, choice of model, weighting indicators and treatment of missing values etc. These judgements should be transparent and based on sound statistical principles.

Notes - Composite indicators typically seek to reduce distinct quality measures into a single summary indicator. The methods underlying such simplifications should be clear and transparent. Too often, however, composite indicators are presented with limited or no information about the derivation and interpretation of constituent measures. The technical information required to understand how composite indicators were designed is sometimes not published5 or is not reported alongside the actual composite indicator. Some measures are used without clear conceptual justification

A key assumption underlying the use of composite indicators is that the constituent parts together give a fair summary of the whole

Many composite indicator schemes apply threshold-based classification rules to standardise disparate individual measures to a consistent scale. Measures that are naturally continuous are mapped to categorical bands before being combined into the overall composite

The weighting assigned to individual measures contributing to composites is another problem area. As few hospitals perform equally well in all areas, performance can be artificially improved by giving higher weight to individual measures where a hospital performs better than average and vice versa. The choice of weights given to individual measures is thus a key determinant of performance on the overall composite, and different weights might allow almost any rank to be achieved.31 32 Therefore, transparency is needed about the importance attached to each measure in terms of the aim of the indicator, with supporting evidence. However, many schemes do not provide explicit justification for the weights used to create the composite 

https://www.medrxiv.org/content/10.1101/2020.12.08.20246256v1.full 

https://qualitysafety.bmj.com/content/28/4/338 
 

---
class: left, middle

# Learning objectives

This session is a tutorial to show you how to go through each steps required to do the calculation correctly.

### Stage 0. Prepare data & information
### Stage 1. Review & Process data
### Stage 2. Question the  assumptions
### Stage 3. Visualize Results
 


???
Depending on the pace of the group, if we do not finish today we will org anise a second session 

---

# Webinar rules

<i class="fa fa-spinner fa-spin fa-fw fa-2x"></i> Leverage this opportunity and make this session __lively__ - there's no stupid questions!

<i class="fa fa-check-square fa-fw fa-2x"></i> Use the __chat__ to send your questions - we are two facilitators and one is focused on replying all questions directly in the chat while the session is on-going

<i class="fa fa-pencil  fa-fw fa-2x"></i> No need to take notes, all the session __content__ will be shared 

<i class="fa fa-cog fa-fw fa-2x"></i> All practical exercises are designed to get you __testing the commands__:

> Login on a dedicated cloud-based version of RStudio with base packages pre-installed for this session @ 
    
> Paste the command from the chat to your online Rstudio session and check what is happening
    
> In case it is not working as expected, share screenshot or error messages from the console in the chat


---

## Follow the recipe


.pull-left[

In the development area,  there has been long focus to improve the quality of index through methodology nd user guide such as 
[Handbook on Constructing Composite Indicators](https://www.oecd.org/sdd/42495745.pdf), the  [Ten Step Guide](https://knowledge4policy.ec.europa.eu/publication/your-10-step-pocket-guide-composite-indicators-scoreboards_en) & the [pocket Guide](https://knowledge4policy.ec.europa.eu/sites/default/files/10-step-pocket-guide-to-composite-indicators-and-scoreboards.pdf) .  

To bring in the humanitarian world, the same level of statistical quality than in development (aka making the "__nexus__"), the following steps are required: 

 * Treat the data (missing value, outliers)
 * Explore the correlations
 * Standardize/normalize the data
 * Define the weight of each indicator
 * Develop an aggregation method
 * Assess robustness & sensitivity


 
]

 
.pull-right[
 
![Handbook](images/handbook.png)  


]

???

Notes -

---

### The COIN Object

A dedicated package, [COINr](https://bluefoxr.github.io/COINrDoc) has been created by the European Joint Research center to facilitate the creation of composite indicators. it includes __all steps & components__ to support the process.

![tidyverse](images/coinR.png)  



???

Notes -

 
---
class: inverse, left, middle

# Stage 0. Prepare data & information


 * Document metadata
 * Specify Hierarchy
 * Organise data in tabular format



---

## Document Indicators: 

.pull-left[

Indicator metadata
]

--

.pull-right[
]


???

Notes -

---


## Describe dimensions and nested levels: 



.pull-left[
Aggregation metadata
 
]

--

.pull-right[
]

???

Notes -

---


## Load data and assemble

.pull-left[
To build a composite indicator,  we need three data frames, prepared and assembled in Excel according to a specific format with predefined column name:

1. Indicator data
2. Indicator metadata (_as described previously_)
3. Aggregation metadata (_as described previously_)
 
]

--

.pull-right[

```{r, collapse=T}
library(dplyr)
library(COINr)
library(reactable)
library(magrittr)
# install ggbiplot if you don't have it
# library(devtools)
# install_github("vqv/ggbiplot")
library(ggbiplot)

ASEM <- assemble(IndData = ASEMIndData,
                 IndMeta = ASEMIndMeta,
                 AggMeta = ASEMAggMeta)
```

]

???

Notes -

 

---
class: inverse, left, middle

# Stage 1. Review & Process data

  1. Indicator statistics 
  
  2. Missing data
  
  3. Denominator
  
  4. Outliers
  
  5. Normalisation
  
  6. Aggregation



---

## Check the indicator statistics



.pull-left[
 
It is advisable to do this both on the raw data, and the normalised/aggregated data.
 

Particular things of interest : 
 * whether indicators or aggregates are highly skewed, 
 * the percentage of missing data for each indicator, 
 * the percentage of unique values, low data availability and zeroes, 
 * the presence of correlations with denominators, and negative correlations 
]

--

.pull-right[

```{r}
# We can change thresholds for flagging outliers and high/low correlations.
ASEM <- getStats(ASEM, dset = "Raw")
# view stats within a table
ASEM$Analysis$Raw$StatTable %>%
  roundDF() %>%
  reactable()
```
]



???


Notes -At this point you may decide to check individual indicators, and some may be added or excluded.

---

## View missing data by group
 


.pull-left[
 

```{r}
ASEM <- checkData(ASEM, dset = "Raw")

# 
ASEM$Analysis$Raw$MissDatByGroup %>%
  roundDF %>%
  reactable::reactable()
``` 
]

--

.pull-right[
]



???

Notes -  

--- 
class: inverse, left, middle

## Treat & Process data



??? 

--- 

## Add Denomination 



.pull-left[
 

To be able to compare small countries with larger ones, you may need to divide indicators by e.g. GDP or population , you can use `denominate()`. The specifications are either made initially in `IndMeta`, or as arguments to `denominate()`. In the case of the ASEM data set, these are included in `IndMeta` so the command is very simple (run `View(ASEMIndMeta)` to see). We will afterwards check the new stats to see what has changed. 
]

--

.pull-right[



```{r}
# create denominated data set
ASEM <- denominate(ASEM, dset = "Raw")

# get stats of denominated data
ASEM <- getStats(ASEM, dset = "Denominated")

# view stats table
ASEM$Analysis$Raw$StatTable %>%
  roundDF() %>%
  reactable()
``` 

]


???

Notes -According to the new table, there are now no high correlations with denominators, which indicates some kind of success.

---


## Treat the data for outliers



.pull-left[
 

Standard approach which Winsorises each indicator up to a specified limit of points, in order to bring skew and kurtosis below specified thresholds. If Winsorisation fails, it applies a log transformation or similar.

```{r}
ASEM <- treat(ASEM, 
              dset = "Denominated", 
              winmax = 5)
``` 
]

--

.pull-right[

Following treatment, it is a good idea to check which indicators were treated and how:

```{r}
ASEM$Analysis$Treated$TreatSummary %>%
  filter(Treatment != "None")
```


]




???

Notes -

---


## Visualise effect of data treatment



.pull-left[
 
Effect of the Winsorisation can be plotted using box plots or violin plots. 
]

--

.pull-right[


```{r}
iplotIndDist2(ASEM, 
              dsets = c("Denominated", "Treated"), 
              icodes = "Services", 
              ptype = "Scatter")
``` 
]


???

Notes -It is also a good idea to visualise and compare the treated data against the untreated data. The best way to do this interactively is to call `indDash()` again, which allows comparison of treated and untreated indicators side by side. 

---


## Normalise the data



.pull-left[
 

The next step would be to normalise the data. In the ASEM index we will use a simple min-max normalisation in the $[0, 100]$ interval. 
]

--

.pull-right[

```{r}
ASEM <- normalise(ASEM, 
                  dset = "Treated", 
                  ntype = "minmax", 
                  npara = list(minmax = c(0,100)))
```

]


???

Notes -Again, we could visualise and check stats here but to keep things shorter we'll skip that for now.

---


## Aggregation of all indicators: which method to use?



.pull-left[
 
Aggregation type can be:
 * 
 * 
]

--

.pull-right[
```{r}
ASEM <- aggregate(ASEM, 
                  agtype = "arith_mean", 
                  dset = "Normalised")

```
]
 


???

Notes -

---
class: inverse, left, middle

# Stage 2. Question the  assumptions


* Correlations between indicators,  with parents pillars,  within and between pillars
* Internal Consistency
* Principle component
* Effect of specific formulation
* Sensitivity Analysis

???

correlation structure between the underlying indicators and its effect on the overall score (i.e., the CI). Ideally, there should be positive correlations between the indicators as this indicates that individual variables are linked to an overarching concept


---

## Reconstruct the index from raw data 



.pull-left[

to check differences, Check calculations for pre-aggregated data to make sure that they are correct. If you only have normalised and aggregated data, you can still at least check the aggregation stage as follows. Assuming that the indicator columns in your pre-aggregated data are normalised, we can first manually create a normalised data set: 
]

--

.pull-right[


```{r, message=FALSE}
# extract aggregated data set (as a data frame)
Aggregated_Data <- ASEM$Data$Aggregated

# assemble new COIN only including pre-aggregated data
ASEM_preagg <- assemble(IndData = Aggregated_Data,
                        IndMeta = ASEMIndMeta,
                        AggMeta = ASEMAggMeta,
                        preagg = TRUE)
```

```{r}
ASEM_preagg$Data$Normalised <- ASEM_preagg$Data$PreAggregated %>%
  select(!ASEM$Input$AggMeta$Code)
```

Here we have just copied the pre-aggregated data, but removed any aggregation columns.

Next, we can aggregate these columns using COINr.

```{r}
ASEM_preagg <- aggregate(ASEM_preagg, 
                         dset = "Normalised", 
                         agtype = "arith_mean")

# check data set names
names(ASEM_preagg$Data)
```

]

???

Notes - COINr will check that the column names in the indicator data correspond to the codes supplied in the `IndMeta` and `AggMeta`. This means that these two latter data frames still need to be supplied. However, from this point the COIN functions as any other, although consider that it cannot be regenerated (the methodology to arrive at the pre-aggregated data is unknown), and the only data set present is the "PreAggregated" data.

---

## Check whether  data frames are the same 



.pull-left[
 

Double-checking calculations is tedious but in the process you often learn a lot.  
]

--

.pull-right[

```{r}
all_equal(ASEM_preagg$Data$PreAggregated,
          ASEM_preagg$Data$Aggregated)
```

]


???

Notes - As expected, here the results are the same. If the results are *not* the same, `all_equal()` will give some information about the differences. If you reconstruct the index from raw data, and you find differences, a few points are worth considering:

1. The difference could be due to an error in the pre-aggregated data, or even a bug in COINr. If you suspect the latter please open an issue on the repo.
2. If you have used data treatment or imputation, differences can easily arise. One reason is that some things are possible to calculate in different ways. COINr uses certain choices, but other choices are also valid. Examples of this include:
    - Skew and kurtosis (underlying data treatment) - see e.g. `?e1071::skewness`
    - Correlation and treatment of missing values - see `?cor`
    - Ranks and how to handle ties - see `?rank`
3. Errors can also arise from how you entered the data. Worth re-checking all that as well.



---



## Revise internal consistency 

Cronbach's alpha calculation should be done for any group of indicators 
```{r}
# all indicators
getCronbach(ASEM, dset = "Normalised")

# indicators in connectivity sub-index
getCronbach(ASEM, dset = "Normalised", icodes = "Conn", aglev = 1)

# indicators in sustainability sub-index
getCronbach(ASEM, dset = "Normalised", icodes = "Sust", aglev = 1)

# pillars in connectivity sub-index
getCronbach(ASEM, dset = "Aggregated", icodes = "Conn", aglev = 2)

# pillars in sustainability sub-index
getCronbach(ASEM, dset = "Aggregated", icodes = "Sust", aglev = 2)
```
???

Notes -Recall here that because we have plotted a "Raw" indicator against the index, and this is a negative indicator, its direction is flipped. Meaning that in this plot there is a positive correlation, but plotting the normalised indicator against the index would show a negative correlation.

---


## Correlation analysis on the normalised data 

As indicators have had their directions reversed where appropriate, how the consistency of the sustainability pillars is then affected? 

```{r}
plotCorr(ASEM, dset = "Aggregated", icodes = "Sust", aglevs = 2, pval = 0)
```

???

Notes - 
Sustainability dimensions are not well-correlated and are in fact slightly negatively correlated. This points to trade-offs between different aspects of sustainable development: as social sustainability increases, environmental sustainability often tends to decrease. Or at best, an increase in one does not really imply an increase in the others.

---


## Principle component analysis 

This can be done only on data without missing value

```{r}


# impute one
ASEM2 <- impute(ASEM, dset = "Denominated", imtype = "indgroup_mean", groupvar = "Group_GDP")
ASEM2 <- normalise(ASEM2, dset = "Treated", ntype = "minmax", npara = list(minmax = c(0,100)))
# try here at the indicator level, let's say within one of the pillar groups:
PCA_P2P <- getPCA(ASEM2, dset = "Normalised", 
                  icodes = "P2P", 
                  aglev = 1, 
                  out2 = "list")

summary(PCA_P2P$PCAresults$P2P$PCAres)
```



```{r}

# ggbiplot(PCA_P2P$PCAresults$P2P$PCAres,
#          labels = ASEM$Data$Normalised$UnitCode,
#          groups = ASEM$Data$Normalised$Group_EurAsia)
```


???

Notes - We can see that the first principle component explains about 50% of the variance of the indicators, which is perhaps borderline for the existence of single latent variable. That said, many composite indicators will not yield strong latent variables in many cases.

We can now produce a PCA biplot using this information.
Once again we see a fairly clear divide between Asia and Europe in terms of P2P connectivity, with exceptions of Singapore which is very well-connected. We also note the small cluster of New Zealand and Australia which have very similar characteristics in P2P connectivity.

---


## Understand the effective weights of each indicator

A sometimes under-appreciated fact is that the weight of an indicator in the final index is due to its own weight, plus the weight of all its parents, as well as the number of indicators and aggregates in each group. For example, an equally-weighted group of two indicators will each have a higher weight (0.5) than an equally weighted group of ten indicators (0.1), and this applies to all aggregation levels.

```{r}
#  `effectiveWeight()` gives effective weights for all levels. We can check the indicator level by filtering:
EffWts <- effectiveWeight(ASEM)
# Filter the maximum and minimum effective weights and which indicators are involved:
# IndWts <- EffWts$EffectiveWeightsList %>%
#   filter(AgLevel == 1) %>%
#   filter(EffectiveWeight %in% c(min(IndWts$EffectiveWeight), max(IndWts$EffectiveWeight)))

plotframework(ASEM)
```

???

Notes -

---


## What-ifs: Effect of  formulation on scores & ranks 

Examples could include different weights, as well as adding/removing/substituting indicators or entire aggregation groups


```{r}
# Copy the COIN
ASEM_NoPolitical <- ASEM

# Copy the weights
ASEM_NoPolitical$Parameters$Weights$NoPolitical <- ASEM_NoPolitical$Parameters$Weights$Original

# Set Political weight to zero
ASEM_NoPolitical$Parameters$Weights$NoPolitical$Weight[
  ASEM_NoPolitical$Parameters$Weights$NoPolitical$Code == "Political"] <- 0

# Alter methodology to use new weights
ASEM_NoPolitical$Method$aggregate$agweights <- "NoPolitical"

# Regenerate
ASEM_NoPolitical <- regen(ASEM_NoPolitical)
```

Now we need to compare the two alternative indexes:

```{r}
compTable(ASEM, ASEM_NoPolitical, dset = "Aggregated", isel = "Index",
          COINnames = c("Original", "NoPolitical"))
```
???

Notes - 
1. Use the "exclude" argument of `assemble()` to exclude the relevant indicators
2. Set the weight of the Political pillar to zero
3. Remove the indicators manually

Here we will take the quickest option, which is Option 2.
The results show that the rank changes are not major at the index level, with a maximum shift of six places for Estonia. The implication might be (depending on context) that the inclusion or not of the Political pillar does not have a drastic impact on the results, although one should bear in mind that the changes on lower aggregation levels are probably higher, and the indicators themselves may have value and add legitimacy to the framework. In other words, it is not always just the index that counts.

---


## Sensitivity analysis:  test the overall effect of uncertainties
 

???

Notes -  

---
class: inverse, left, middle

# Stage 3. Visualize Results

* Index
* SubDimensions
* Maps
* Summary
* Export


---

## Index & Sub Dimensions

We can now visualize our results. A good way at the index level is a stacked bar chart.

```{r}
iplotBar(ASEM, dset = "Aggregated", 
         isel = "Index", 
         aglev = 4, 
         stack_children = T)
```
???

Notes -

---


## Sub Indices Interactions

 

```{r}
iplotIndDist2(ASEM, 
              dsets = "Aggregated", 
              icodes = "Index")
```

???

Notes -

---


## Display the index on a Map

 
```{r}
iplotMap(ASEM, dset = "Aggregated", isel = "Conn")
```

???

Notes -

---


## Present summary table.

```{r}
getResults(ASEM, tab_type = "Summary") %>%
  knitr::kable()
```


???

Notes -

---


## Export Results

Get tables to present (the highest levels of aggregation are the first columns, rather than the last, and it is sorted by index score).

```{r, eval=F}
# Write full results table to COIN
getResults(ASEM, tab_type = "FullWithDenoms", out2 = "COIN")

# Export entire COIN to Excel
coin2Excel(ASEM, "ASEM_results.xlsx")
```


???

Notes -

---
class: inverse, left, middle

# Conclusion. Reproducibility to ensure transparency

 * Strictly Use "curated & published" data for your sub-indicators
 
 * Put Code into github to document your process - the data structure required for the packages allows for better discoverability!
 
 * Call for peer review to cover yourself! 

